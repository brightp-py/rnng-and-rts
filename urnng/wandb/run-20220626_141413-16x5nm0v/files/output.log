C:\Users\Brighton\anaconda3\envs\myenv\lib\site-packages\torch\nn\functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
C:\Users\Brighton\anaconda3\envs\myenv\lib\site-packages\torch\nn\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
ElboPPL: 100.53, ReconPPL: 64.90, KL: 6.2283, IwaePPL: 100.06, CorpusF1: 38.07, SentAvgF1: 31.80
Starting epoch 1
Epoch: 1, Batch: 500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1437, TrainVAEPPL: 90.65, TrainReconPPL: 56.24, TrainKL: 6.39, TrainIWAEPPL: 56.43, |Param|: 910.54, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.0145, GoldTreeF1: 46.42, Throughput: 5.84 examples/sec
PRED: ((Uh ,) (and (he was)))
GOLD: (Uh (, (and (he was))))
Epoch: 1, Batch: 1000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1263, TrainVAEPPL: 90.73, TrainReconPPL: 56.31, TrainKL: 6.47, TrainIWAEPPL: 56.70, |Param|: 913.25, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.0289, GoldTreeF1: 46.24, Throughput: 5.91 examples/sec
PRED: (but ((I ((was still) (in (high school)))) ,))
GOLD: (but (I ((was (still (in (high school)))) ,)))
Epoch: 1, Batch: 1500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1246, TrainVAEPPL: 91.57, TrainReconPPL: 56.79, TrainKL: 6.56, TrainIWAEPPL: 57.39, |Param|: 915.98, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.0434, GoldTreeF1: 45.88, Throughput: 5.78 examples/sec
PRED: (and (then (you (can (make chocolate)))))
GOLD: ((and then) (you (can (make chocolate))))
Epoch: 1, Batch: 2000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1181, TrainVAEPPL: 90.48, TrainReconPPL: 56.05, TrainKL: 6.58, TrainIWAEPPL: 56.84, |Param|: 918.82, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.0579, GoldTreeF1: 46.09, Throughput: 5.84 examples/sec
PRED: ((there (were (three (of (us (who went)))))) .)
GOLD: (there ((were (three ((of us) (who went)))) .))
Epoch: 1, Batch: 2500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1130, TrainVAEPPL: 89.99, TrainReconPPL: 55.73, TrainKL: 6.60, TrainIWAEPPL: 56.71, |Param|: 921.67, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.0724, GoldTreeF1: 46.07, Throughput: 5.83 examples/sec
PRED: ((Acting (((only (as (interpreter Carla))) (, ((her (hands folded)) (on (her lap))))) (, (was (utterly impersonal))))) .)
GOLD: ((Acting (only (as interpreter))) ((Carla (, ((her hands) (folded (on (her lap)))))) (, ((was (utterly impersonal)) .))))
Epoch: 1, Batch: 3000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1133, TrainVAEPPL: 90.12, TrainReconPPL: 55.76, TrainKL: 6.59, TrainIWAEPPL: 56.94, |Param|: 924.30, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.0869, GoldTreeF1: 45.94, Throughput: 5.85 examples/sec
PRED: ((At ((one point) (, ((the (Dow (Jones (Industrial (average (fell (about (80 points)))))))) (on (news (that (UAL (Corp. (decided (to (remain independent)))))))))))) .)
GOLD: ((At (one point)) (, ((the (Dow (Jones (Industrial average)))) ((fell (((about 80) points) (on (news (that ((UAL Corp.) (decided (to (remain independent))))))))) .))))
Epoch: 1, Batch: 3500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1146, TrainVAEPPL: 90.41, TrainReconPPL: 55.89, TrainKL: 6.69, TrainIWAEPPL: 57.29, |Param|: 927.22, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.1015, GoldTreeF1: 45.86, Throughput: 5.78 examples/sec
PRED: ((When ((((a (David (Baltimore (-- (or (the (next (target (-- (decides (it (is (better (to (stand (up (to (these forces)))))))))))))))))) (, (his (fellow (scientists (would (do (well (to (recognize (what (is (fundamentally (at stake)))))))))))))) ,) (and (offer (their (public support)))))) .)
GOLD: ((When (((a (David Baltimore)) (-- (or ((the (next target)) --)))) (decides (it (is (better (to (stand (up (to (these forces))))))))))) (, ((his (fellow scientists)) ((would (do (well (to ((recognize (what (is (fundamentally (at stake))))) (, (and (offer (their (public support)))))))))) .))))
Epoch: 1, Batch: 4000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1158, TrainVAEPPL: 90.40, TrainReconPPL: 55.90, TrainKL: 6.73, TrainIWAEPPL: 57.50, |Param|: 929.97, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.1160, GoldTreeF1: 45.67, Throughput: 5.69 examples/sec
PRED: (Pay (him (('' !) !)))
GOLD: ((Pay him) ('' (! !)))
Epoch: 1, Batch: 4500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1135, TrainVAEPPL: 89.93, TrainReconPPL: 55.57, TrainKL: 6.70, TrainIWAEPPL: 57.36, |Param|: 932.64, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.1306, GoldTreeF1: 45.72, Throughput: 5.63 examples/sec
PRED: ((There are) (, ((((he thought) (, (so (few (true (means ((of forgetfulness) (in ((this life) (that (why (should (he (shun (the (medicine (even (when (the (medicine seemed)))))))))))))))))))) (, (as (it did)))) (, (((a (little crude)) ?) ?)))))
GOLD: (There ((are (, ((he thought) (, ((((so few) (true means)) (of forgetfulness)) ((in (this life)) (that (why (should (he (shun ((the medicine) (even (when ((the medicine) (seemed (, ((as (it did)) (, (a (little crude))))))))))))))))))))) (? ?)))
Epoch: 1, Batch: 5000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1123, TrainVAEPPL: 90.12, TrainReconPPL: 55.67, TrainKL: 6.75, TrainIWAEPPL: 57.67, |Param|: 935.41, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.1451, GoldTreeF1: 45.56, Throughput: 5.49 examples/sec
PRED: ((I (went (where (I (wanted (to go)))))) .)
GOLD: (I ((went (where (I (wanted (to go))))) .))
Epoch: 1, Batch: 5500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1117, TrainVAEPPL: 89.92, TrainReconPPL: 55.58, TrainKL: 6.76, TrainIWAEPPL: 57.78, |Param|: 938.16, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.1595, GoldTreeF1: 45.54, Throughput: 5.49 examples/sec
PRED: ((It (was ((a (((three-month journey) (in (the (dead (of (winter followed)))))) (by (three (months (of labor)))))) (on (Mackinac boats))))) .)
GOLD: (It ((was ((a (three-month journey)) ((in ((the dead) (of winter))) (followed (by ((three months) (of (labor (on (Mackinac boats)))))))))) .))
Epoch: 1, Batch: 6000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1103, TrainVAEPPL: 89.50, TrainReconPPL: 55.35, TrainKL: 6.73, TrainIWAEPPL: 57.73, |Param|: 940.81, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.1740, GoldTreeF1: 45.59, Throughput: 5.49 examples/sec
PRED: (But (, (uh ,)))
GOLD: (But (, (uh ,)))
Epoch: 1, Batch: 6500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1104, TrainVAEPPL: 89.32, TrainReconPPL: 55.27, TrainKL: 6.73, TrainIWAEPPL: 57.84, |Param|: 943.47, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.1885, GoldTreeF1: 45.61, Throughput: 5.51 examples/sec
PRED: ((The ((1988 (figures (were (restated (to (include (the (results (of (Lorimar (Telepictures Corp.))))))))))) (, (which ((Warner acquired) (in January)))))) .)
GOLD: ((The (1988 figures)) ((were (restated (to (include ((the results) (of ((Lorimar (Telepictures Corp.)) (, (which (Warner (acquired (in January)))))))))))) .))
Epoch: 1, Batch: 7000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1094, TrainVAEPPL: 88.98, TrainReconPPL: 55.09, TrainKL: 6.71, TrainIWAEPPL: 57.84, |Param|: 946.09, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.2029, GoldTreeF1: 45.58, Throughput: 5.51 examples/sec
PRED: (Oh (, (really ?)))
GOLD: (Oh (, (really ?)))
Epoch: 1, Batch: 7500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1100, TrainVAEPPL: 89.10, TrainReconPPL: 55.20, TrainKL: 6.73, TrainIWAEPPL: 58.16, |Param|: 948.87, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.2174, GoldTreeF1: 45.54, Throughput: 5.48 examples/sec
PRED: ((this (must (be (a (hard category))))) .)
GOLD: (this ((must (be (a (hard category)))) .))
Epoch: 1, Batch: 8000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1106, TrainVAEPPL: 88.90, TrainReconPPL: 55.10, TrainKL: 6.74, TrainIWAEPPL: 58.26, |Param|: 951.57, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.2319, GoldTreeF1: 45.49, Throughput: 5.46 examples/sec
PRED: (and (((((it seems) (in (the (last (decade (or two)))))) (, (um ,))) (that ('s true))) ,))
GOLD: (and (it ((seems ((in (the (last (decade (or two))))) (, (um (, (that ('s true))))))) ,)))
Epoch: 1, Batch: 8500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1102, TrainVAEPPL: 88.73, TrainReconPPL: 55.02, TrainKL: 6.75, TrainIWAEPPL: 58.38, |Param|: 954.24, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.2463, GoldTreeF1: 45.49, Throughput: 5.45 examples/sec
PRED: ((If (((((you (find sound)) (, ((healthy companies) (in Japan)))) (, (they (are (not (for sale)))))) (, ('' (said (George Watanabe))))) (, (a (<unk> (at (Tokyo-based (Asia (Advisory (Services Inc)))))))))) .)
GOLD: (((If (you (find ((sound (, (healthy companies))) (in Japan))))) (, (they (are (not (for sale)))))) (, ('' (said (((George Watanabe) (, ((a <unk>) (at (Tokyo-based (Asia (Advisory (Services Inc)))))))) .)))))
Epoch: 1, Batch: 9000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1099, TrainVAEPPL: 88.55, TrainReconPPL: 54.93, TrainKL: 6.75, TrainIWAEPPL: 58.48, |Param|: 956.95, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.2608, GoldTreeF1: 45.49, Throughput: 5.45 examples/sec
PRED: (((And ((I think) (that (the (people (who ((are strongly) (in (favor (of (the (death (penalty (are (really ((working from) (that (gut level)))))))))))))))))) (. (((((((Uh (, ((you know) (, (whether (it (be (a (biblical force))))))))) (, ((uh (, (you know))) (, (the (eye (for (an eye)))))))) (, (a (tooth (for (a tooth)))))) (, (a (life (for (a (life (type (logic (or just)))))))))) (, (uh (, (uh (, (some (sort (of (anger (at putting))))))))))) (, (uh (, ((murderers up) (in (federal (pens (for (the (rest (of (their life))))))))))))) (, (uh (, (while (we (foot (the bill)))))))))) .)
GOLD: (And (I ((think (that (((the people) (who (are (strongly (in (favor (of (the (death penalty))))))))) (are (really (working ((from (that (gut level))) (. (Uh ((, ((you know) ,)) (whether (it (be (((a (biblical force)) (, (uh ((, ((you know) ,)) (the ((eye (for (an eye))) (, ((a (tooth (for (a tooth)))) (, ((a (life (for (a life)))) (type logic))))))))))) (or (just (, (uh (, (uh (, ((some sort) (of (anger (at (putting (, (uh (, (murderers (up ((in (federal pens)) ((for ((the rest) (of (their life)))) (, (uh (, (while (we (foot (the bill)))))))))))))))))))))))))))))))))))))))) .)))
Epoch: 1, Batch: 9500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1101, TrainVAEPPL: 88.54, TrainReconPPL: 54.95, TrainKL: 6.76, TrainIWAEPPL: 58.70, |Param|: 959.60, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.2753, GoldTreeF1: 45.49, Throughput: 5.44 examples/sec
PRED: ((Then ((off again) (, (rushing (to (keep up)))))) .)
GOLD: (Then (off (again (, ((rushing (to (keep up))) .)))))
Epoch: 1, Batch: 10000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1102, TrainVAEPPL: 88.24, TrainReconPPL: 54.79, TrainKL: 6.74, TrainIWAEPPL: 58.72, |Param|: 962.33, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.2898, GoldTreeF1: 45.51, Throughput: 5.47 examples/sec
PRED: ((oh (, absolutely)) ,)
GOLD: (oh (, (absolutely ,)))
Epoch: 1, Batch: 10500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1107, TrainVAEPPL: 88.17, TrainReconPPL: 54.77, TrainKL: 6.74, TrainIWAEPPL: 58.90, |Param|: 965.11, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.3043, GoldTreeF1: 45.49, Throughput: 5.47 examples/sec
PRED: ((Well (, (get (((your pa) (and get)) (that (whisky '')))))) .)
GOLD: (Well (, (((get (your pa)) (and (get (that whisky)))) ('' .))))
Epoch: 1, Batch: 11000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1108, TrainVAEPPL: 87.96, TrainReconPPL: 54.68, TrainKL: 6.73, TrainIWAEPPL: 58.98, |Param|: 967.77, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.3189, GoldTreeF1: 45.49, Throughput: 5.46 examples/sec
PRED: ((they did) .)
GOLD: (they (did .))
Epoch: 1, Batch: 11500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1121, TrainVAEPPL: 87.77, TrainReconPPL: 54.60, TrainKL: 6.72, TrainIWAEPPL: 59.09, |Param|: 970.45, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.3334, GoldTreeF1: 45.52, Throughput: 5.48 examples/sec
PRED: (`` ((You ((<unk> <unk>) ?)) ?))
GOLD: (`` (You ((<unk> <unk>) (? ?))))
Epoch: 1, Batch: 12000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1132, TrainVAEPPL: 87.83, TrainReconPPL: 54.67, TrainKL: 6.71, TrainIWAEPPL: 59.36, |Param|: 973.07, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.3479, GoldTreeF1: 45.49, Throughput: 5.47 examples/sec
PRED: ((Kind (of neighborhood)) .)
GOLD: (Kind ((of neighborhood) .))
Epoch: 1, Batch: 12500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1149, TrainVAEPPL: 87.76, TrainReconPPL: 54.65, TrainKL: 6.70, TrainIWAEPPL: 59.53, |Param|: 975.59, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.3624, GoldTreeF1: 45.50, Throughput: 5.49 examples/sec
PRED: ((All right) .)
GOLD: (All (right .))
Epoch: 1, Batch: 13000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1166, TrainVAEPPL: 87.74, TrainReconPPL: 54.68, TrainKL: 6.68, TrainIWAEPPL: 59.74, |Param|: 978.13, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.3769, GoldTreeF1: 45.50, Throughput: 5.48 examples/sec
PRED: (Uh-huh .)
GOLD: (Uh-huh .)
Epoch: 1, Batch: 13500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1178, TrainVAEPPL: 87.60, TrainReconPPL: 54.64, TrainKL: 6.66, TrainIWAEPPL: 59.88, |Param|: 980.68, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.3914, GoldTreeF1: 45.51, Throughput: 5.49 examples/sec
PRED: ((they (do (n't (really (pay (a (whole (lot (of (attention (to <unk>))))))))))) .)
GOLD: (they ((do (n't (really (pay (((a (whole lot)) (of attention)) (to <unk>)))))) .))
Epoch: 1, Batch: 14000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1187, TrainVAEPPL: 87.49, TrainReconPPL: 54.60, TrainKL: 6.65, TrainIWAEPPL: 60.02, |Param|: 983.26, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.4059, GoldTreeF1: 45.53, Throughput: 5.49 examples/sec
PRED: (have (you ((recycled (plastic also)) ?)))
GOLD: (have (you ((recycled (plastic also)) ?)))
Epoch: 1, Batch: 14500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1204, TrainVAEPPL: 87.35, TrainReconPPL: 54.54, TrainKL: 6.63, TrainIWAEPPL: 60.14, |Param|: 985.85, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.4204, GoldTreeF1: 45.54, Throughput: 5.51 examples/sec
PRED: (but ((he ('s (really (from France)))) ,))
GOLD: (but (he (('s (really (from France))) ,)))
Epoch: 1, Batch: 15000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1223, TrainVAEPPL: 87.30, TrainReconPPL: 54.55, TrainKL: 6.62, TrainIWAEPPL: 60.34, |Param|: 988.46, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.4349, GoldTreeF1: 45.52, Throughput: 5.50 examples/sec
PRED: (Uh-huh .)
GOLD: (Uh-huh .)
Epoch: 1, Batch: 15500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1254, TrainVAEPPL: 87.24, TrainReconPPL: 54.55, TrainKL: 6.62, TrainIWAEPPL: 60.54, |Param|: 991.06, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.4494, GoldTreeF1: 45.51, Throughput: 5.47 examples/sec
PRED: ((In (our ((((age (of Science)) (and (Angst (it (seems (to (me (more (brave (to stay)))))))))) (on Earth)) (and (explore (inner (man (than (to (fly (far (from (the (sphere ((of (our sorrow)) (and (explore (outer (space ''))))))))))))))))))) .)
GOLD: ((In ((our age) (of (Science (and Angst))))) (it ((seems ((to me) ((more brave) ((to ((stay (on Earth)) (and (explore (inner man))))) (than (to ((fly (far (from ((the sphere) (of (our sorrow)))))) (and (explore (outer space)))))))))) ('' .))))
Epoch: 1, Batch: 16000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1283, TrainVAEPPL: 87.10, TrainReconPPL: 54.50, TrainKL: 6.62, TrainIWAEPPL: 60.68, |Param|: 993.55, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.4639, GoldTreeF1: 45.51, Throughput: 5.46 examples/sec
PRED: ((I ('m (((an (electrical engineer)) (by (trade ((uh ,) here)))) (in Virginia)))) .)
GOLD: (I (('m ((an (electrical engineer)) ((by trade) (uh (, (here (in Virginia))))))) .))
Epoch: 1, Batch: 16500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1315, TrainVAEPPL: 87.09, TrainReconPPL: 54.52, TrainKL: 6.62, TrainIWAEPPL: 60.90, |Param|: 996.12, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.4785, GoldTreeF1: 45.49, Throughput: 5.46 examples/sec
PRED: (Hewlett ((Packard makes) ,))
GOLD: ((Hewlett Packard) (makes ,))
Epoch: 1, Batch: 17000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.1339, TrainVAEPPL: 86.99, TrainReconPPL: 54.49, TrainKL: 6.61, TrainIWAEPPL: 61.06, |Param|: 998.63, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.4930, GoldTreeF1: 45.48, Throughput: 5.47 examples/sec
PRED: ((On (((Sept. 13) (, (Japan (began (a (policy (of (screening (boat people))))))))) (, (accepting (only (those (deemed (to (be (political refugees)))))))))) .)
GOLD: ((On (Sept. 13)) (, (Japan ((began (((a policy) (of (screening (boat people)))) (, (accepting ((only those) (deemed (to (be (political refugees))))))))) .))))
--------------------------------
Checking validation perf...
ElboPPL: 100.84, ReconPPL: 64.99, KL: 6.2518, IwaePPL: 100.42, CorpusF1: 38.12, SentAvgF1: 31.83
--------------------------------
Starting epoch 2
Epoch: 2, Batch: 500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.2140, TrainVAEPPL: 106.05, TrainReconPPL: 66.85, TrainKL: 6.98, TrainIWAEPPL: 84.49, |Param|: 1001.63, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.5146, GoldTreeF1: 45.44, Throughput: 7.59 examples/sec
PRED: ((did ((n't (have (to (have (lots (of (expensive (machinery to)))))))) (, (uh (, ((get (your heating)) (and (cooling (cycles (to work)))))))))) .)
GOLD: ((did (n't (have ((to (have (lots (of (expensive machinery))))) (to (, (uh (, (get ((your (heating (and (cooling cycles)))) (to work))))))))))) .)
Epoch: 2, Batch: 1000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.2090, TrainVAEPPL: 105.61, TrainReconPPL: 66.56, TrainKL: 6.86, TrainIWAEPPL: 84.40, |Param|: 1003.43, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.5290, GoldTreeF1: 45.43, Throughput: 7.78 examples/sec
PRED: (((Since then) (, (((Moon ('s (organization (has (inaugurated (a (pair (of (high-quality (glossy (opinion magazines))))))))))) (, (((The World) (and I)) (and Insight)))) (, (which (are (a (further drain)))))))) .)
GOLD: ((Since then) (, (((Moon 's) organization) ((has (inaugurated (((a pair) (of (high-quality (glossy (opinion magazines))))) (, ((((The World) (and I)) (and Insight)) (, (which (are (a (further drain)))))))))) .))))
Epoch: 2, Batch: 1500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.2141, TrainVAEPPL: 105.22, TrainReconPPL: 66.38, TrainKL: 6.87, TrainIWAEPPL: 84.41, |Param|: 1005.30, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.5435, GoldTreeF1: 45.42, Throughput: 7.82 examples/sec
PRED: (Uh-huh .)
GOLD: (Uh-huh .)
Epoch: 2, Batch: 2000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.2135, TrainVAEPPL: 104.73, TrainReconPPL: 65.99, TrainKL: 6.81, TrainIWAEPPL: 84.25, |Param|: 1007.10, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.5579, GoldTreeF1: 45.43, Throughput: 7.87 examples/sec
PRED: (and (((there ('s (no (mountains (or anything))))) (, (you know))) ,))
GOLD: (and (there (('s ((no (mountains (or anything))) (, (you know)))) ,)))
Epoch: 2, Batch: 2500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.2128, TrainVAEPPL: 104.25, TrainReconPPL: 65.63, TrainKL: 6.78, TrainIWAEPPL: 84.10, |Param|: 1009.01, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.5725, GoldTreeF1: 45.44, Throughput: 7.93 examples/sec
PRED: ((we ((do (have (the (health (care (uh ,)))))) (and (the dental)))) .)
GOLD: (we ((do (have ((the (health care)) (uh (, (and (the dental))))))) .))
Epoch: 2, Batch: 3000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.2152, TrainVAEPPL: 104.89, TrainReconPPL: 66.04, TrainKL: 6.77, TrainIWAEPPL: 84.91, |Param|: 1010.75, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.5869, GoldTreeF1: 45.42, Throughput: 7.89 examples/sec
PRED: (Yeah .)
GOLD: (Yeah .)
Epoch: 2, Batch: 3500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.2180, TrainVAEPPL: 104.75, TrainReconPPL: 66.02, TrainKL: 6.77, TrainIWAEPPL: 85.12, |Param|: 1012.51, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.6015, GoldTreeF1: 45.41, Throughput: 7.89 examples/sec
PRED: (Uh-huh .)
GOLD: (Uh-huh .)
Epoch: 2, Batch: 4000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.2174, TrainVAEPPL: 104.25, TrainReconPPL: 65.70, TrainKL: 6.72, TrainIWAEPPL: 84.98, |Param|: 1014.30, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.6160, GoldTreeF1: 45.42, Throughput: 7.98 examples/sec
PRED: ((It ('s (one (of (our (favorite movies)))))) .)
GOLD: (It (('s (one (of (our (favorite movies))))) .))
Epoch: 2, Batch: 4500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.2174, TrainVAEPPL: 104.19, TrainReconPPL: 65.65, TrainKL: 6.67, TrainIWAEPPL: 85.20, |Param|: 1015.97, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.6305, GoldTreeF1: 45.43, Throughput: 8.06 examples/sec
PRED: (Well (, (((I (really (did (enjoy it)))) (, (uh (, (you know))))) ,)))
GOLD: (Well (, (I (really ((did (enjoy (it (, (uh (, (you know))))))) ,)))))
Epoch: 2, Batch: 5000/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.2193, TrainVAEPPL: 104.46, TrainReconPPL: 65.82, TrainKL: 6.66, TrainIWAEPPL: 85.71, |Param|: 1017.56, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.6450, GoldTreeF1: 45.42, Throughput: 8.08 examples/sec
PRED: (Yeah ,)
GOLD: (Yeah ,)
Epoch: 2, Batch: 5500/17309, LR: 1.0000, qLR: 0.00010, qEnt: 0.2199, TrainVAEPPL: 104.56, TrainReconPPL: 65.87, TrainKL: 6.63, TrainIWAEPPL: 86.05, |Param|: 1019.27, BestValPerf: 100.06, BestValF1: 38.07, KLPen: 0.6594, GoldTreeF1: 45.43, Throughput: 8.15 examples/sec
PRED: (and ((I ((put them) (in (this home)))) ,))
GOLD: (and (I ((put (them (in (this home)))) ,)))
Traceback (most recent call last):
  File "C:\git\rnng-and-rts\urnng\train.py", line 349, in <module>
    main(args)
  File "C:\git\rnng-and-rts\urnng\train.py", line 191, in main
    (-obj.mean()).backward()
  File "C:\Users\Brighton\anaconda3\envs\myenv\lib\site-packages\torch\_tensor.py", line 255, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "C:\Users\Brighton\anaconda3\envs\myenv\lib\site-packages\torch\autograd\__init__.py", line 147, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt